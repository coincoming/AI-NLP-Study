# -*- coding: utf-8 -*-

import torch
import json
from tabulate import tabulate
import random
import os
import numpy as np
import logging
from config import Config
from model import TorchModel, choose_optimizer
from evaluateDiy import Evaluator
from loaderDiy import DataGeneratorDiy,load_data
logging.basicConfig(level=logging.INFO, format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

"""
模型训练主程序
"""


seed = Config["seed"] 
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)

def main(config):
    #创建保存模型的目录
    if not os.path.isdir(config["model_path"]):
        os.mkdir(config["model_path"])
    # 加载训练数据集，测试数据集
    dg = DataGeneratorDiy(Config)
    #加载模型
    model = TorchModel(config)
    # 标识是否使用gpu
    cuda_flag = torch.cuda.is_available()
    if cuda_flag:
        logger.info("gpu可以使用，迁移模型至gpu")
        model = model.cuda()
    #加载优化器
    optimizer = choose_optimizer(config, model)
    #加载效果测试类
    evaluator = Evaluator(config, model, logger,load_data(dg.get_train_data(),config,True),load_data(dg.get_test_data(),config,False))
    #训练
    for epoch in range(config["epoch"]):
        epoch += 1
        model.train()
        logger.info("epoch %d begin" % epoch)
        train_loss = []
        for index, batch_data in enumerate(load_data(dg.get_train_data(),config,True)):
            if cuda_flag:
                batch_data = [d.cuda() for d in batch_data]
            optimizer.zero_grad()
            input_ids, labels = batch_data   #输入变化时这里需要修改，比如多输入，多输出的情况
            loss = model(input_ids, labels)
            loss.backward()
            optimizer.step()

            train_loss.append(loss.item())
            if index % int(len(dg.get_train_data()) / 2) == 0:
                logger.info("batch loss %f" % loss)
        logger.info("epoch average loss: %f" % np.mean(train_loss))
        train_data_acc = evaluator.eval(epoch)
    #计算测试集下的准确率
    acc, spend_time,test_data_size  = evaluator.eval_train_set()
    Config["test_data_acc"] = acc
    Config["test_data_spend_time"] = spend_time
    Config["test_data_size"] = test_data_size
    Config["time_per_100"] = spend_time / test_data_size * 100 * 1000 # 计算的是每100条预测样例的耗时，单位为 ms
    return acc

if __name__ == "__main__":
    #对比所有模型
    #中间日志可以关掉，避免输出过多信息
    # 超参数的网格搜索
    result = []
    for model in ['lstm','rnn','bert_lstm']:
        Config["model_type"] = model
        for lr in [1e-3, 1e-4]:
            Config["learning_rate"] = lr
            for hidden_size in [128]:
                Config["hidden_size"] = hidden_size
                for batch_size in [64, 128]:
                    Config["batch_size"] = batch_size
                    for pooling_style in ["avg", 'max']:
                        Config["pooling_style"] = pooling_style
                        main(Config)
                        result.append(json.dumps(Config, default=lambda obj: int(obj) if isinstance(obj, np.int64) else obj))
    # 字符串解为字典
    dict_result = []
    for config in result:
        dict_result.append(json.loads(config))
    #输出字典
    # columnNames = list(dict_result[0].keys()) # 获取字典的键,如果要输出所有配置信息的胡
    columnNames = ["model_type","max_length","hidden_size","kernel_size","num_layers","epoch","batch_size","pooling_style  ","optimizer  ","learning_rate","seed","train_data_percent","class_num","vocab_size","train_data_distribute  ","test_data_distribute  ","test_data_acc","test_data_spend_time","test_data_size","time_per_100" ]
    table = []
    for dict in dict_result:
        table.append([dict[columnName] for columnName in columnNames])
    print(tabulate(table, headers= columnNames, tablefmt="grid"))
# 本地测试的结果情况
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| model_type   |   max_length |   hidden_size |   kernel_size |   num_layers |   epoch |   batch_size | pooling_style   | optimizer   |   learning_rate |   seed |   train_data_percent |   class_num |   vocab_size | train_data_distribute   | test_data_distribute   |   test_data_acc |   test_data_spend_time |   test_data_size |   time_per_100 |
#+==============+==============+===============+===============+==============+=========+==============+=================+=============+=================+========+======================+=============+==============+=========================+========================+=================+========================+==================+================+
#| lstm         |           30 |           128 |             3 |            2 |      15 |           64 | avg             | adam        |          0.001  |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.987261 |              0.0614877 |              314 |        19.5821 |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| lstm         |           30 |           128 |             3 |            2 |      15 |           64 | max             | adam        |          0.001  |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.987261 |              0.0790555 |              314 |        25.1769 |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| lstm         |           30 |           128 |             3 |            2 |      15 |          128 | avg             | adam        |          0.001  |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.977707 |              0.0546494 |              314 |        17.4043 |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| lstm         |           30 |           128 |             3 |            2 |      15 |          128 | max             | adam        |          0.001  |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.977707 |              0.0867496 |              314 |        27.6272 |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| lstm         |           30 |           128 |             3 |            2 |      15 |           64 | avg             | adam        |          0.0001 |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.786624 |              0.0634387 |              314 |        20.2034 |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| lstm         |           30 |           128 |             3 |            2 |      15 |           64 | max             | adam        |          0.0001 |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.789809 |              0.0772529 |              314 |        24.6028 |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| lstm         |           30 |           128 |             3 |            2 |      15 |          128 | avg             | adam        |          0.0001 |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.757962 |              0.0721085 |              314 |        22.9645 |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| lstm         |           30 |           128 |             3 |            2 |      15 |          128 | max             | adam        |          0.0001 |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.767516 |              0.0692966 |              314 |        22.069  |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| rnn          |           30 |           128 |             3 |            2 |      15 |           64 | avg             | adam        |          0.001  |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.984076 |              0.039048  |              314 |        12.4357 |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| rnn          |           30 |           128 |             3 |            2 |      15 |           64 | max             | adam        |          0.001  |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.990446 |              0.0409925 |              314 |        13.0549 |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| rnn          |           30 |           128 |             3 |            2 |      15 |          128 | avg             | adam        |          0.001  |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.961783 |              0.0351365 |              314 |        11.19   |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| rnn          |           30 |           128 |             3 |            2 |      15 |          128 | max             | adam        |          0.001  |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.984076 |              0.0478218 |              314 |        15.2299 |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| rnn          |           30 |           128 |             3 |            2 |      15 |           64 | avg             | adam        |          0.0001 |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.850318 |              0.0423682 |              314 |        13.493  |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| rnn          |           30 |           128 |             3 |            2 |      15 |           64 | max             | adam        |          0.0001 |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.929936 |              0.0361118 |              314 |        11.5006 |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| rnn          |           30 |           128 |             3 |            2 |      15 |          128 | avg             | adam        |          0.0001 |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.770701 |              0.0429363 |              314 |        13.674  |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| rnn          |           30 |           128 |             3 |            2 |      15 |          128 | max             | adam        |          0.0001 |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.894904 |              0.0526962 |              314 |        16.7822 |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| bert_lstm    |           30 |           128 |             3 |            2 |      15 |           64 | avg             | adam        |          0.001  |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.490446 |             12.3843    |              314 |      3944.05   |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| bert_lstm    |           30 |           128 |             3 |            2 |      15 |           64 | max             | adam        |          0.001  |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.509554 |             12.5735    |              314 |      4004.3    |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| bert_lstm    |           30 |           128 |             3 |            2 |      15 |          128 | avg             | adam        |          0.001  |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.509554 |             12.3887    |              314 |      3945.45   |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| bert_lstm    |           30 |           128 |             3 |            2 |      15 |          128 | max             | adam        |          0.001  |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.509554 |             12.5472    |              314 |      3995.91   |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| bert_lstm    |           30 |           128 |             3 |            2 |      15 |           64 | avg             | adam        |          0.0001 |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.888535 |             13.3979    |              314 |      4266.85   |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| bert_lstm    |           30 |           128 |             3 |            2 |      15 |           64 | max             | adam        |          0.0001 |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.977707 |             12.6579    |              314 |      4031.18   |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| bert_lstm    |           30 |           128 |             3 |            2 |      15 |          128 | avg             | adam        |          0.0001 |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.977707 |             12.3279    |              314 |      3926.1    |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+
#| bert_lstm    |           30 |           128 |             3 |            2 |      15 |          128 | max             | adam        |          0.0001 |    987 |                  0.8 |           2 |         4622 | [191, 202]              | [37, 42]               |        0.977707 |             12.3801    |              314 |      3942.7    |
#+--------------+--------------+---------------+---------------+--------------+---------+--------------+-----------------+-------------+-----------------+--------+----------------------+-------------+--------------+-------------------------+------------------------+-----------------+------------------------+------------------+----------------+